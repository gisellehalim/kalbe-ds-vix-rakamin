# -*- coding: utf-8 -*-
"""Kalbe_Nutritionals_ARIMA_VIX.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C4VHjYiAm3TvufIj_g8SEGPnf_rYDFBb

##**Machine Learning for Kalbe Nutritionals**

Objective : Predicting Qty based on past data with ARIMA

###**Attribute information:**

1. Customer

- CustomerID : No Unik Customer
- Age : Usia Customer
- Gender : 0 Wanita, 1 Pria
- Marital Status : Married, Single (Blm menikah/Pernah menikah)
- Income : Pendapatan per bulan dalam jutaan rupiah


2. Store

- StoreID : Kode Unik Store
- StoreName : Nama Toko
- GroupStore : Nama group
- Type : Modern Trade, General Trade
- Latitude : Kode Latitude
- Longitude : Kode Longitude


3. Product

- ProductID : Kode Unik Product
- Product Name : Nama Product
- Price : Harga dlm rupiah


4. Transaction

- TransactionID : Kode Unik Transaksi
- Date : Tanggal transaksi
- Qty : Jumlah item yang dibeli
- Total Amount : Price x Qty

# Import Library dan Load Data
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = 15, 6
import seaborn as sns
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.tsa.arima.model import ARIMA
import warnings
from sklearn.metrics import mean_squared_error
from math import sqrt

"""### Load Dataset"""

df_cus = pd.read_csv('Customer.csv')
df_str = pd.read_csv('Store.csv')
df_prod = pd.read_csv('Product.csv')
df_trans = pd.read_csv('Transaction.csv')

"""### Data Preprocessing

#### Customer
"""

df_cus = df_cus.drop_duplicates()

#Looking at the first 5 rows of the dataset
df_cus.head()

#Looking at the last 5 rows of the dataset
df_cus.tail()

#How many rows and columns in the dataset?
df_cus.shape

#Labeling categorical data
status = {
    "Single": 0,
    "Married": 1
}

df_cus['MaritalStatus'] = df_cus['MaritalStatus'].map(status)

df_cus.head()

#General information of the dataset
df_cus.info()

"""##### Handling Missing Values"""

#Checking for missing values
df_cus.isnull().sum()

#filling missing data
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean', missing_values=np.nan)
imputer = imputer.fit(df_cus[['MaritalStatus']])
df_cus['MaritalStatus'] = imputer.transform(df_cus[['MaritalStatus']])

#Checking if the data is still missing
df_cus.isnull().sum()

"""#### Store"""

df_str = df_str.drop_duplicates()

#Looking at the first 5 rows of the dataset
df_str.head()

#Looking at the last 5 rows of the dataset
df_str.tail()

#How many rows and columns in the dataset?
df_str.shape

#General information of the dataset
df_str.info()

"""##### Handling Missing Values"""

#Checking for missing values
df_str.isnull().sum()

"""#### Product"""

df_prod = df_prod.drop_duplicates()

#Looking at the first 5 rows of the dataset
df_prod.head()

#Looking at the last 5 rows of the dataset
df_str.tail()

#How many rows and columns in the dataset?
df_prod.shape

#General information of the dataset
df_prod.info()

"""##### Handling Missing Values"""

#Checking for missing values
df_prod.isnull().sum()

"""#### Transaction"""

df_trans = df_trans.drop_duplicates()

#Looking at the first 5 rows of the dataset
df_trans.head()

#Looking at the last 5 rows of the dataset
df_trans.tail()

#How many rows and columns in the dataset?
df_trans.shape

#General information of the dataset
df_trans.info()

"""##### Handling Missing Values"""

#Checking for missing values
df_trans.isnull().sum()

"""### Merging Dataset"""

df1 = df_trans.merge(df_cus, on='CustomerID', how='inner')
df2 = df1.merge(df_str, on='StoreID', how='inner')
df = df2.merge(df_prod.drop(columns=['Price']), on='ProductID', how='inner')

df.head()

df.describe()

df = df.groupby('Date').agg({'Qty': 'sum'}).reset_index()

df.head()

#Describing the dataset
df.describe()

"""# Data Exploration"""

df.isnull().sum()/len(df)

print(df.dtypes)

#Changing date to index
con = df['Date']
df['Date']=pd.to_datetime(df['Date'], dayfirst=True)
df.set_index('Date')
#check datatype of index
df.index

df

#Qty as time series variable
ts = df['Qty']
ts.head(10)

#Dataset visualization
plt.figure(figsize=(20,10))
sns.set_style('darkgrid')
plt.xlabel('Day-N')
plt.ylabel('Qty')
plt.title('Penjualan Harian')
plt.plot(df['Qty'])

"""## Uji Stasioneritas Data"""

#Deklarasi fungsi untuk mengecek stasioneritas data
def test_stationarity(timeseries):

    #Determing rolling statistics
    rolmean = timeseries.rolling(window=12).mean()
    rolstd = timeseries.rolling(window=12).std()

    #Plot rolling statistics:
    orig = plt.plot(timeseries, color='blue',label='Original')
    mean = plt.plot(rolmean, color='red', label='Rolling Mean')
    std = plt.plot(rolstd, color='black', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.title('Rolling Mean & Standard Deviation')
    plt.show()


    #Perform Dickey-Fuller test:
    print('Results of Dickey-Fuller Test:')
    dftest = adfuller(timeseries, autolag='AIC')
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
        dfoutput['Critical Value (%s)'%key] = value
    print(dfoutput)

"""Uji Hipotesis:

H0 : Data tidak stasioner

Ha : Data stasioner

Tolak H0 apabila diperoleh p-value < alpha (0,05)
"""

#Melakukan uji stasioneritas untuk dataset
test_stationarity(ts)

"""Keadaan data stasioner karena p-value < 0.05"""

plt.figure()
plt.subplot(211)
plot_acf(ts, ax=plt.gca(), lags=100)
plt.subplot(212)
plot_pacf(ts, ax=plt.gca(), lags=100)
plt.show()

"""Plot ACF dan plot PACF dibentuk untuk mengetahui orde dari AR dan MA, sehingga bisa diketahui model yang sesuai untuk data."""

#Splitting Data 80:20
train, test = df[:293], df[293:]

y = train['Qty']

#ARIMA
ARIMAmodel = ARIMA(y, order = (40, 2, 1))
ARIMAmodel = ARIMAmodel.fit()

y_pred = ARIMAmodel.get_forecast(len(test))

y_pred_df = y_pred.conf_int()
y_pred_df['predictions'] = ARIMAmodel.predict(start = y_pred_df.index[0], end = y_pred_df.index[-1])
y_pred_df.index = test.index
y_pred_out = y_pred_df['predictions']

plt.figure(figsize=(20,5))
plt.title("Daily Sales Quantity Prediction")
plt.plot(train['Qty'])
plt.plot(test['Qty'], color='green')
plt.xlabel("Day-N")
plt.ylabel("Qty")
plt.plot(y_pred_out, color='black', label='ARIMA Predictions')
plt.legend()

from sklearn.metrics import mean_absolute_error, mean_squared_error

y_actual = test['Qty']

# Calculating the metrics
mae = mean_absolute_error(y_actual, y_pred_out)
mse = mean_squared_error(y_actual, y_pred_out)
rmse = np.sqrt(mse)
mape = np.mean(np.abs((y_actual - y_pred_out) / y_actual)) * 100

# Print the evaluation metrics
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")

ARIMAmodel.summary()

forecast_steps = 30
forecast = ARIMAmodel.forecast(steps=forecast_steps)

print('Forecasted Values:')
print(forecast)